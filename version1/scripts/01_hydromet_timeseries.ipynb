{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydrometeorological time series\n",
    "\n",
    "Notebook to create a .csv file with the hydrometeorological time series data for all CAMELS-DE catchments.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from camelsp import get_metadata, Station\n",
    "from camelsp.util import OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create folder structure\n",
    "\n",
    "Inspired by CAMELS-UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder camels_de\n",
    "os.makedirs(\"../output_data/camels_de\", exist_ok=True)\n",
    "\n",
    "# create subfolder timeseries\n",
    "os.makedirs(\"../output_data/camels_de/timeseries\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make selection of stations\n",
    "\n",
    "- At least 10 years of discharge data **-> we have to controll this again after trimming the data to 1951-2021**\n",
    "- MERIT Hydro catchment geometry available\n",
    "- catchment area greater than 5 km² and smaller than 15000 km²\n",
    "- catchment area located entirely within Germany\n",
    "- catchment area max difference to reported area 20 percent\n",
    "- all HYRAS variables available "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preselection criteria from metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations after preselection: 1621\n"
     ]
    }
   ],
   "source": [
    "# get metadata\n",
    "metadata = get_metadata()\n",
    "\n",
    "# preselect stations\n",
    "metadata = metadata[\n",
    "    (metadata[\"q_more_than_10_years\"]) &\n",
    "    (metadata[\"merit_hydro_available\"] == True) &\n",
    "    (metadata[\"merit_area_greater_5_smaller_15000\"] == True) &\n",
    "    (metadata[\"merit_completely_within_germany\"] == True) &\n",
    "    (metadata[\"merit_difference_to_reported_area_smaller_20_percent\"] == True)\n",
    "    ]\n",
    "\n",
    "print(f\"Number of stations after preselection: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYRAS availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations after HYRAS availability check: 1600\n"
     ]
    }
   ],
   "source": [
    "# check availability of all HYRAS variables\n",
    "hyras_available = pd.read_csv(os.path.join(OUTPUT_PATH, \"scripts/meteo/hyras/hyras_exact_extract_availability.csv\"), index_col=0)\n",
    "\n",
    "# check where all columns are True\n",
    "hyras_available = hyras_available.all(axis=\"columns\")\n",
    "\n",
    "# only keep stations where all columns are True\n",
    "hyras_available = hyras_available[hyras_available]\n",
    "\n",
    "# compare with metadata\n",
    "metadata = metadata[metadata[\"camels_id\"].isin(hyras_available.index)]\n",
    "\n",
    "print(f\"Number of stations after HYRAS availability check: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Q > 10 years after trimming to 1951-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations after threshold check: 1595\n"
     ]
    }
   ],
   "source": [
    "# set threshold for minimum number of days / data points\n",
    "threshold = 365*10\n",
    "\n",
    "q_more_than_10_years = []\n",
    "\n",
    "for camels_id in metadata[\"camels_id\"]:\n",
    "    # initialize station\n",
    "    s = Station(camels_id)\n",
    "\n",
    "    # get data\n",
    "    df = s.get_data(date_index=False)\n",
    "\n",
    "    # trim to start and end date\n",
    "    start_date, end_date = \"1951-01-01\", \"2020-12-31\"\n",
    "    df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
    "\n",
    "    # count q values that are not nan\n",
    "    if 'q' in df.columns:\n",
    "        q_count = df['q'].count()\n",
    "\n",
    "        if q_count > threshold:\n",
    "            q_more_than_10_years.append(camels_id)\n",
    "\n",
    "# compare with metadata\n",
    "metadata = metadata[metadata[\"camels_id\"].isin(q_more_than_10_years)]\n",
    "\n",
    "print(f\"Number of stations after threshold check: {len(q_more_than_10_years)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual inspection / expert knowledge\n",
    "\n",
    "By manual inspection of the data and incorporation of expert knowledge, the following stations are excluded from CAMELS-DE, as the data seems to be of low quality: \n",
    "- DE410590\n",
    "- DEA10420\n",
    "- DEE10300\n",
    "- DEA10410\n",
    "- DED10250\n",
    "- DE410290\n",
    "- DE811090\n",
    "- DE910540\n",
    "- DEA10430\n",
    "- DED10250\n",
    "- DE410600\n",
    "- DEA11030\n",
    "- DE411010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations after exclusion: 1583\n"
     ]
    }
   ],
   "source": [
    "exclude = [\"DE410590\", \"DEA10420\", \"DEE10300\", \"DEA10410\", \"DED10250\", \"DE410290\", \"DE811090\", \"DE910540\", \"DEA10430\", \"DED10250\", \"DE410600\", \"DEA11030\", \"DE411010\"]\n",
    "\n",
    "# exclude stations from metadata\n",
    "metadata = metadata[~metadata[\"camels_id\"].isin(exclude)]\n",
    "\n",
    "print(f\"Number of stations after exclusion: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stations in CAMELS-DE v1: 1583\n"
     ]
    }
   ],
   "source": [
    "camels_ids = metadata[\"camels_id\"]\n",
    "\n",
    "print(f\"Total number of stations in CAMELS-DE v1: {len(camels_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge HYRAS with q/w data and save as .csv\n",
    "\n",
    "For some stations, there are duplicated dates in the q/w data. We will remove these duplicates by taking the mean of the duplicates if the difference of the max and min q/w values to the mean q v/w value is smaller than 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_out_duplicated_dates(df_qw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Helper function to average out duplicated dates in a DataFrame with columns \n",
    "    \"date\", \"q\" and \"w\".\n",
    "    Averaging only happens if thev relative difference of the max and min value \n",
    "    of q and w to the mean value is smaller than 0.05. Otherwise, a ValueError \n",
    "    is raised.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_qw : pd.DataFrame\n",
    "        DataFrame with columns \"date\", \"q\" and \"w\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with averaged out duplicated dates.\n",
    "\n",
    "    \"\"\"\n",
    "    # get duplicated dates\n",
    "    duplicated_dates = df_qw[df_qw[\"date\"].duplicated()][\"date\"].unique()\n",
    "\n",
    "    # loop over duplicated dates, if duplicates are within 0.05 of each other, take the mean to remove duplicates\n",
    "    for duplicated_date in duplicated_dates:\n",
    "        # get min, max and mean of q for duplicated date\n",
    "        min_q = df_qw[df_qw[\"date\"] == duplicated_date][\"q\"].min()\n",
    "        max_q = df_qw[df_qw[\"date\"] == duplicated_date][\"q\"].max()\n",
    "        mean_q = df_qw[df_qw[\"date\"] == duplicated_date][\"q\"].mean()\n",
    "\n",
    "        # get min, max and mean of w for duplicated date\n",
    "        min_w = df_qw[df_qw[\"date\"] == duplicated_date][\"w\"].min()\n",
    "        max_w = df_qw[df_qw[\"date\"] == duplicated_date][\"w\"].max()\n",
    "        mean_w = df_qw[df_qw[\"date\"] == duplicated_date][\"w\"].mean()\n",
    "        \n",
    "        # check if relative differences of max and min to mean are smaller than 0.05\n",
    "        if (abs(max_q - mean_q) / mean_q < 0.05) and (abs(min_q - mean_q) / mean_q < 0.05) and (abs(max_w - mean_w) / mean_w < 0.05) and (abs(min_w - mean_w) / mean_w < 0.05):\n",
    "            # take mean of q and w\n",
    "            df_qw = df_qw.groupby(\"date\").mean().reset_index()\n",
    "        else:\n",
    "            raise ValueError(f\"Difference of max and min to mean q/w values for {duplicated_date} is too large.\")\n",
    "            \n",
    "    return df_qw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 249/1460 [01:52<09:26,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate dates in DED10480\n",
      "Fixed duplicates in DED10480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 930/1460 [08:06<11:19,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate dates in DE410160\n",
      "Fixed duplicates in DE410160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 951/1460 [09:29<03:52,  2.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate dates in DE411150\n",
      "Fixed duplicates in DE411150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 952/1460 [09:29<04:04,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate dates in DE411170\n",
      "Fixed duplicates in DE411170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [14:14<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated CAMELS-DE v1 hydrometeorological timeseries data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# list of HYRAS variables\n",
    "hyras_variables = [\"Precipitation\", \"Humidity\", \"RadiationGlobal\", \"TemperatureMean\", \"TemperatureMin\", \"TemperatureMax\"]\n",
    "\n",
    "for camels_id in camels_ids:\n",
    "    # initialize station\n",
    "    s = Station(camels_id)\n",
    "\n",
    "    # get data\n",
    "    df_qw = s.get_data(date_index=False)\n",
    "    \n",
    "    # make sure date column is datetime\n",
    "    df_qw[\"date\"] = pd.to_datetime(df_qw[\"date\"])\n",
    "\n",
    "    # drop q_flag and w_flag columns if they exist\n",
    "    if \"q_flag\" in df_qw.columns:\n",
    "        df_qw.drop(columns=\"q_flag\", inplace=True)\n",
    "    if \"w_flag\" in df_qw.columns:\n",
    "        df_qw.drop(columns=\"w_flag\", inplace=True)\n",
    "\n",
    "    # check for duplicates in date column\n",
    "    if df_qw[\"date\"].duplicated().any():\n",
    "        print(f\"Duplicate dates in {camels_id}\")\n",
    "\n",
    "        # remove duplicates if difference is below threshold, raises ValueError if difference at duplicated date is too large\n",
    "        df_qw = average_out_duplicated_dates(df_qw)\n",
    "\n",
    "        print(f\"Fixed duplicates in {camels_id}\")\n",
    "        \n",
    "    # rename columns q and w, if they exist\n",
    "    if \"q\" in df_qw.columns:\n",
    "        df_qw.rename(columns={\"q\": \"discharge_vol\"}, inplace=True)\n",
    "\n",
    "    if \"w\" in df_qw.columns:\n",
    "        df_qw.rename(columns={\"w\": \"water_level\"}, inplace=True)\n",
    "    # else: add nan water_level column\n",
    "    else:\n",
    "        df_qw[\"water_level\"] = None\n",
    "\n",
    "    # calculate specific discharge with area from merit hydro\n",
    "    catchment = s.get_catchment(\"merit_hydro\")\n",
    "    area = catchment.to_crs(\"EPSG:6933\").area.values[0]  # in m^2\n",
    "    df_qw[\"discharge_spec\"] = ((df_qw[\"discharge_vol\"] * 86400) / area) * 1000  # in mm/day\n",
    "\n",
    "    # reorder columns\n",
    "    df_qw = df_qw[[\"date\", \"discharge_vol\", \"discharge_spec\", \"water_level\"]]\n",
    "\n",
    "    # read HYRAS data\n",
    "    for variable in hyras_variables:\n",
    "        # read hyras data\n",
    "        df_hyras = pd.read_csv(os.path.join(s.output_path, \"hyras\", f\"{camels_id}_{variable}.csv\"))\n",
    "\n",
    "        # rename first column to date\n",
    "        df_hyras.rename(columns={df_hyras.columns[0]: \"date\"}, inplace=True)\n",
    "\n",
    "        # drop time from datetime date column in df_hyras\n",
    "        df_hyras[\"date\"] = pd.to_datetime(df_hyras[\"date\"]).dt.date\n",
    "\n",
    "        # make sure date columns are datetime before merging\n",
    "        df_hyras[\"date\"] = pd.to_datetime(df_hyras[\"date\"])\n",
    "\n",
    "        # merge data, keep all HYRAS data to make sure date range is correct\n",
    "        df_qw = pd.merge(df_qw, df_hyras, on=\"date\", how=\"right\")\n",
    "\n",
    "    # drop temperature columns\n",
    "    df_qw.drop(columns=[\"temperature_mean_min\", \"temperature_mean_median\", \"temperature_mean_max\", \"temperature_mean_stdev\",\n",
    "                        \"temperature_min_min\", \"temperature_min_median\", \"temperature_min_max\", \"temperature_min_stdev\",\n",
    "                        \"temperature_max_min\", \"temperature_max_median\", \"temperature_max_max\", \"temperature_max_stdev\"], inplace=True)\n",
    "\n",
    "    # rename columns\n",
    "    df_qw.rename(columns={\"temperature_mean_mean\": \"temperature_mean\", \"temperature_min_mean\": \"temperature_min\", \"temperature_max_mean\": \"temperature_max\"}, inplace=True)\n",
    "    \n",
    "    # round to 2 decimal places\n",
    "    df_qw = df_qw.round(2)\n",
    "\n",
    "    # save to csv\n",
    "    df_qw.to_csv(f\"../output_data/camels_de/timeseries/CAMELS_DE_hydromet_timeseries_{camels_id}.csv\", index=False)\n",
    "\n",
    "print(\"Generated CAMELS-DE v1 hydrometeorological timeseries data!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
